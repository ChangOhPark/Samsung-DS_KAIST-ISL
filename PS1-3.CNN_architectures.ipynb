{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center\"> CNN Architectures </div>\n",
    "#### <div style=\"text-align: right\"> 2019.09.19 Thursday </div>\n",
    "#### <div style=\"text-align: right\"> Prof. Changho Suh, TA Jaewoong Cho, Junhyung Ahn</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**실습목표**\n",
    "- 수업시간에 배운 AlexNet, ResNet 생성\n",
    "- CIFAR10 Data 불러오기, 데이터 확인 \n",
    "- Validation 및 Early stopping 개념 이해\n",
    "- AlexNet 과 ResNet에서 CIFAR10 training\n",
    "- 성능비교 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.datasets import cifar10, cifar100\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras.datasets.cifar import load_batch\n",
    "from tensorflow.python.keras.utils.data_utils import get_file\n",
    "from tensorflow.python.util.tf_export import keras_export\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smooth_batch_losses(array):\n",
    "    smoothing_alpha = 0.95\n",
    "    output = []\n",
    "    value = 0\n",
    "    for i in range(len(array)):\n",
    "        value = smoothing_alpha * value + (1 - smoothing_alpha) * array[i]\n",
    "        output.append(value / (1 - smoothing_alpha**(i+1)))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(cnn_batch_stats):\n",
    "    plt.figure(figsize = (15,15))\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.ylabel(\"Loss\", fontsize=15)\n",
    "    plt.xlabel(\"Training Steps\", fontsize=15)\n",
    "    plt.ylim([0,2])\n",
    "    plt.plot(get_smooth_batch_losses(cnn_batch_stats.batch_losses))\n",
    "\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.ylabel(\"Train Accuracy\", fontsize=15)\n",
    "    plt.xlabel(\"Training Steps\", fontsize=15)\n",
    "    plt.ylim([0,1])\n",
    "    plt.plot(get_smooth_batch_losses(cnn_batch_stats.batch_acc))\n",
    "\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.ylabel(\"Loss\", fontsize=15)\n",
    "    plt.xlabel(\"Epochs\" , fontsize=15)\n",
    "    plt.ylim([0,3])\n",
    "    plt.plot(cnn_batch_stats.epoch_train_loss, label = 'train loss per epoch')\n",
    "    plt.plot(cnn_batch_stats.epoch_val_loss, label = 'validation loss per epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2,2,4)\n",
    "    plt.ylabel(\"Accuracy\", fontsize=15)\n",
    "    plt.xlabel(\"Epochs\", fontsize=15)\n",
    "    plt.ylim([0,1])\n",
    "    plt.plot(cnn_batch_stats.epoch_train_acc, label = 'train accuracy per epoch')\n",
    "    plt.plot(cnn_batch_stats.epoch_val_acc, label = 'validation accuracy per epoch')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For logging\n",
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.batch_losses = []\n",
    "        self.batch_acc = []\n",
    "        self.epoch_train_loss = []\n",
    "        self.epoch_train_acc = []\n",
    "        self.epoch_val_loss = []\n",
    "        self.epoch_val_acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        self.batch_losses.append(logs['loss'])\n",
    "        self.batch_acc.append(logs['acc'])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_train_loss.append(logs['loss'])\n",
    "        self.epoch_train_acc.append(logs['acc'])\n",
    "        self.epoch_val_loss.append(logs['val_loss'])\n",
    "        self.epoch_val_acc.append(logs['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "    if epoch == 30 or epoch ==60 : \n",
    "        lr = lr  * 0.1\n",
    "    else:\n",
    "        lr = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1) AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# AleXNet without Batch normalization, Dropout layer  \n",
    "alexnet_model = tf.keras.models.Sequential()\n",
    "\n",
    "#1. First Layer (Convolutional Layer)\n",
    "alexnet_model.add(tf.keras.layers.Conv2D(filters=48, input_shape=(32,32,3), kernel_size=(3,3), \n",
    "                                         strides= (1,1), padding='same'))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "alexnet_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),padding = 'valid'))\n",
    "\n",
    "#2. Second Layer (Convolutional Layer)\n",
    "alexnet_model.add(tf.keras.layers.Conv2D(filters=96, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "alexnet_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),padding = 'valid'))\n",
    "\n",
    "#3. Third Layer (Convolutional Layer)\n",
    "alexnet_model.add(tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "#4. Fourth Layer (Convolutional Layer)\n",
    "alexnet_model.add(tf.keras.layers.Conv2D(filters=192, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "#5. Fifth Layer (Convolutional Layer)\n",
    "alexnet_model.add(tf.keras.layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='same'))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "alexnet_model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2), strides=(2,2),padding = 'valid'))\n",
    "\n",
    "#6. Sixth Layer (Fully Connected Layer)\n",
    "alexnet_model.add(tf.keras.layers.Flatten())\n",
    "alexnet_model.add(tf.keras.layers.Dense(512))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "#7. Seventh Layer (Fully Connected Layer)\n",
    "alexnet_model.add(tf.keras.layers.Dense(256))\n",
    "alexnet_model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "#8. Eighth Layer (Fully Connected Layer)\n",
    "alexnet_model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "alexnet_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2) ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNet\n",
    "input = keras.layers.Input(shape=(32, 32, 3))\n",
    "x = keras.layers.Conv2D(32, kernel_size=3, padding='same', strides=(1, 1))(input)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Conv2D(32, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D(2)(x)\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "skip = x\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Conv2D(64, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip])\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.MaxPool2D(2)(x)\n",
    "x = keras.layers.Conv2D(128, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "skip = x\n",
    "x = keras.layers.Conv2D(128, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.Conv2D(128, kernel_size=3, padding='same', strides=(1, 1))(x)\n",
    "x = keras.layers.BatchNormalization()(x)\n",
    "x = keras.layers.Add()([x, skip])\n",
    "x = keras.layers.ReLU()(x)\n",
    "x = keras.layers.AveragePooling2D(pool_size=(8, 8))(x)\n",
    "x = keras.layers.Flatten()(x)\n",
    "\n",
    "# Fully-connected layer\n",
    "output = keras.layers.Dense(10, activation=tf.nn.softmax)(x)\n",
    "resnet = keras.models.Model(input, output)\n",
    "\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'alexnet': alexnet_model , 'resnet': resnet}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 1/5, stratify=y_train)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "class_names = ['airplane','automobile',\n",
    "                       'bird','cat',\n",
    "                       'deer','dog',\n",
    "                       'frog','horse',\n",
    "                       'ship','truck']\n",
    "\n",
    "fig = plt.figure(figsize=(8,3))\n",
    "\n",
    "for i in range(num_classes):\n",
    "    ax = fig.add_subplot(2, 5, 1 + i, xticks=[], yticks=[])\n",
    "    idx = np.where(y_train[:]==i)[0]\n",
    "    features_idx = x_train[idx]\n",
    "    img_num = np.random.randint(features_idx.shape[0])\n",
    "    im = features_idx[img_num]\n",
    "    ax.set_title(class_names[i])\n",
    "    plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1) AlexNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment setting\n",
    "LRSchedule_flag = False #True or False\n",
    "learning_rate = 0.001\n",
    "momentum=0.9\n",
    "batch_size = 128\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'alexnet'\n",
    "cnn_model = models[model_name]\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#model save folder path\n",
    "model_save_folder_path = './model/'\n",
    "if not os.path.exists(model_save_folder_path):\n",
    "    os.mkdir(model_save_folder_path)\n",
    "\n",
    "model_save_folder_path = model_save_folder_path+model_name\n",
    "if not os.path.exists(model_save_folder_path):\n",
    "    os.mkdir(model_save_folder_path)\n",
    "    \n",
    "model_path = model_save_folder_path+'/best_model.hdf5'\n",
    "\n",
    "#collect callback\n",
    "cnn_batch_stats = CollectBatchStats()\n",
    "#checkpoint callback\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, \n",
    "                                monitor='val_acc',\n",
    "                                verbose=1, \n",
    "                                save_best_only=True)\n",
    "#early stopping callback\n",
    "cb_early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "#learning rate schedule callback\n",
    "cb_learning_rate_schedule = LearningRateScheduler(scheduler)\n",
    "\n",
    "if LRSchedule_flag:\n",
    "    callbacks = [cnn_batch_stats, cb_checkpoint, cb_learning_rate_schedule]\n",
    "else:\n",
    "    callbacks = [cnn_batch_stats, cb_checkpoint, cb_early_stopping]\n",
    "    \n",
    "# Train CNN model\n",
    "cnn_model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cnn_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = cnn_model.evaluate(x_test, y_test)\n",
    "print('Test acc = ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model(model_path)\n",
    "test_loss, test_acc = saved_model.evaluate(x_test, y_test)\n",
    "print('Test acc = ', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2) ResNet training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment Setting\n",
    "LRSchedule_flag = False #True or False\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "batch_size = 128\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = 'resnet'\n",
    "cnn_model = models[model_name]\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=momentum),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "#model save folder path\n",
    "model_save_folder_path = './model/'\n",
    "if not os.path.exists(model_save_folder_path):\n",
    "    os.mkdir(model_save_folder_path)\n",
    "\n",
    "model_save_folder_path = model_save_folder_path+model_name\n",
    "if not os.path.exists(model_save_folder_path):\n",
    "    os.mkdir(model_save_folder_path)\n",
    "    \n",
    "model_path = model_save_folder_path+'/best_model.hdf5'\n",
    "\n",
    "#collect callback\n",
    "cnn_batch_stats = CollectBatchStats()\n",
    "#checkpoint callback\n",
    "cb_checkpoint = ModelCheckpoint(filepath=model_path, \n",
    "                                monitor='val_acc',\n",
    "                                verbose=1, \n",
    "                                save_best_only=True)\n",
    "#early stopping callback\n",
    "cb_early_stopping = EarlyStopping(monitor='val_acc', patience=20)\n",
    "#learning rate schedule callback\n",
    "cb_learning_rate_schedule = LearningRateScheduler(scheduler)\n",
    "\n",
    "if LRSchedule_flag:\n",
    "    callbacks = [cnn_batch_stats, cb_checkpoint, cb_learning_rate_schedule]\n",
    "else:\n",
    "    callbacks = [cnn_batch_stats, cb_checkpoint, cb_early_stopping]\n",
    "    \n",
    "# Train CNN model\n",
    "cnn_model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,epochs=epochs,\n",
    "              callbacks=callbacks,\n",
    "              validation_data=(x_val, y_val),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(cnn_batch_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = cnn_model.evaluate(x_test, y_test)\n",
    "print('Test acc = ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = load_model(model_path)\n",
    "test_loss, test_acc = saved_model.evaluate(x_test, y_test)\n",
    "print('Test acc = ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
